{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Link Prediction with FastRP Embeddings\n",
    "## TigerGraph ML Team\n",
    "\n",
    "We will use FastRP embeddings of vertices to predict if there is a REVIEW edge between Users and Books."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to Database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pyTigerGraph as tg \n",
    "\n",
    "conn = tg.TigerGraphConnection(host=\"http://104.155.129.195\", graphname=\"Amazon_OGB_Test\")\n",
    "embeddingDim = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gather Data\n",
    "Here, we run a query to get both user and book embeddings for all edges in the graph. We have to batch the query because the REST endpoint and timeout limits."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {\n",
    "    \"groups\": 128,\n",
    "    \"groupId\": 0,\n",
    "}\n",
    "\n",
    "users = []\n",
    "books = []\n",
    "ratings = []\n",
    "\n",
    "for i in range(0, 70):\n",
    "    print(i)\n",
    "    params[\"groupId\"] = i\n",
    "    res = conn.runInstalledQuery(\"get_data\", params=params)\n",
    "    users += res[0][\"@@userEmbeddings\"]\n",
    "    books += res[0][\"@@bookEmbeddings\"]\n",
    "    ratings += res[0][\"@@ratings\"]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Negative Samples\n",
    "In order to have a proper dataset for classification, we need to generate negative samples. Negative samples are datapoints that are combinations of book and user embeddings that are not valid edges in the graph. To do this, we generate random shuffles of the book and user embeddings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "positiveSamples = np.append(np.array(res[0][\"@@userEmbeddings\"]), np.array(res[0][\"@@bookEmbeddings\"]), axis=1)\n",
    "negativeSamples = np.append(np.array(rd.sample(res[0][\"@@userEmbeddings\"], k=len(res[0][\"@@userEmbeddings\"]))), np.array(rd.sample(res[0][\"@@bookEmbeddings\"], k=len(res[0][\"@@bookEmbeddings\"]))), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "positiveSamples.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "negativeSamples.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish Dataset Prep\n",
    "We have a dataset that now contains both positive and negative samples. First, create the labels for the dataset. Finally, we can split the dataset into train and test sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = np.append(np.ones(positiveSamples.shape[0]), np.zeros(negativeSamples.shape[0]))\n",
    "X = np.append(positiveSamples, negativeSamples, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Logistic Regression Classification\n",
    "We can now perform logistic regression classification on the dataset. This doesn't perform that well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Look at PCA representation\n",
    "We can now look at the PCA representation of the dataset. Notably, the classes are not linearly seperable. Maybe a neural net would perform better?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_red = pca.fit_transform(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plot = plt.scatter(X_red[:,0], X_red[:,1], c=y)\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=[\"Edge\", \"No Edge\"])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_red = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_red[:,0], X_red[:,1], X_red[:,2], c=y)\n",
    "ax.legend(handles=plot.legend_elements()[0], labels=[\"Edge\", \"No Edge\"])\n",
    "plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network\n",
    "We can now use a neural network to perform classification. This performs **way** better than logistic regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(256, 512, 5), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('scotus': conda)"
  },
  "interpreter": {
   "hash": "c7d6c6bf3a9871602155f511049beaabedd6a9be8dcd8fb85b899fcd7dc30501"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}